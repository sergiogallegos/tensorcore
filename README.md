# TensorCore - Educational Machine Learning Library

A C++ machine learning library designed for educational purposes to understand the core mathematics and implementations behind popular ML libraries like NumPy, PyTorch, and TensorFlow.

## ğŸ¯ Project Goals

- **Educational Focus**: Learn the fundamental mathematics and algorithms behind modern ML libraries
- **Performance**: Implement efficient C++ code with SIMD optimizations and BLAS integration
- **Python Integration**: Provide seamless Python bindings for easy experimentation
- **Transparency**: Well-documented code showing exactly what happens under the hood
- **Modularity**: Clean, modular design that's easy to understand and extend

## âœ… Current Status

### **What's Working Now**
- âœ… **Core Tensor Operations**: Creation, manipulation, arithmetic, shape operations
- âœ… **Mathematical Functions**: 50+ mathematical operations (sin, cos, exp, log, etc.)
- âœ… **Activation Functions**: 15+ activation functions (ReLU, Sigmoid, Tanh, Softmax, etc.)
- âœ… **Loss Functions**: 10+ loss functions (MSE, MAE, Cross-Entropy, etc.)
- âœ… **Automatic Differentiation**: Complete computational graph with forward/backward passes
- âœ… **Neural Network Layers**: Dense, Conv2D, MaxPool2D, AvgPool2D with proper gradients
- âœ… **Optimizers**: SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax, Nadam
- âœ… **Sequential Networks**: Multi-layer neural networks with end-to-end training
- âœ… **SIMD Optimizations**: AVX2/AVX/SSE vectorized operations for maximum performance
- âœ… **Memory Pool**: Efficient allocation/deallocation system for large tensors
- âœ… **Comprehensive Testing**: All core functionality tests passing
- âœ… **Documentation**: Complete API documentation with mathematical explanations

### **What's Coming Next**
- ğŸš§ **Advanced Layers**: LSTM, Transformer, Attention mechanisms
- ğŸš§ **Model Serialization**: Save/load trained models (JSON/Protobuf)
- ğŸš§ **Python Bindings**: Full Python integration for easy experimentation
- ğŸš§ **GPU Acceleration**: CUDA integration for GPU computing
- ğŸš§ **Distributed Training**: Multi-GPU and multi-node support

*See [Development Roadmap](README_ROADMAP.md) for complete feature list and timeline.*

## ğŸ—ï¸ Architecture Overview

### Core Components

- **Tensor Operations**: Multi-dimensional array operations with broadcasting
- **Mathematical Functions**: Linear algebra, statistics, and numerical operations
- **Activation Functions**: ReLU, Sigmoid, Tanh, and other activation functions
- **Loss Functions**: MSE, Cross-Entropy, and other common loss functions
- **Optimizers**: SGD, Adam, RMSprop, and other optimization algorithms
- **Neural Network Layers**: Dense, Convolutional, and other layer types

### Key Features

- **SIMD Optimizations**: AVX2/AVX/SSE vectorized operations for 4x-8x performance boost
- **Memory Pool**: Efficient allocation/deallocation system for large tensors
- **Automatic Differentiation**: Complete computational graph with forward/backward passes
- **Neural Network Layers**: Dense, Conv2D, MaxPool2D, AvgPool2D with proper gradients
- **Optimization Algorithms**: 8 different optimizers (SGD, Adam, RMSprop, etc.)
- **CPU Feature Detection**: Automatic detection of available SIMD instructions
- **Performance Benchmarking**: Comprehensive testing framework
- **GPU Support**: CUDA integration for GPU acceleration (future)

## ğŸ“ Project Structure

```
tensorcore/
â”œâ”€â”€ include/tensorcore/          # Public C++ headers
â”‚   â”œâ”€â”€ tensor.hpp              # Core tensor class
â”‚   â”œâ”€â”€ operations.hpp          # Mathematical operations
â”‚   â”œâ”€â”€ activations.hpp         # Activation functions
â”‚   â”œâ”€â”€ losses.hpp              # Loss functions
â”‚   â”œâ”€â”€ optimizers.hpp          # Optimization algorithms
â”‚   â”œâ”€â”€ layers.hpp              # Neural network layers
â”‚   â”œâ”€â”€ autograd.hpp            # Automatic differentiation
â”‚   â”œâ”€â”€ simd_utils.hpp          # SIMD optimizations
â”‚   â”œâ”€â”€ memory_pool.hpp         # Memory management
â”‚   â””â”€â”€ utils.hpp               # Utility functions
â”œâ”€â”€ src/                        # C++ implementation
â”‚   â”œâ”€â”€ tensor.cpp
â”‚   â”œâ”€â”€ operations.cpp
â”‚   â”œâ”€â”€ activations.cpp
â”‚   â”œâ”€â”€ losses.cpp
â”‚   â”œâ”€â”€ optimizers.cpp
â”‚   â”œâ”€â”€ layers.cpp
â”‚   â”œâ”€â”€ autograd.cpp
â”‚   â”œâ”€â”€ simd_utils.cpp          # SIMD optimizations
â”‚   â””â”€â”€ memory_pool.cpp         # Memory management
â”œâ”€â”€ python/                     # Python bindings
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tensorcore_core.cpp     # pybind11 bindings
â”‚   â”œâ”€â”€ setup.py               # Build configuration
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ tests/                      # Unit tests
â”‚   â”œâ”€â”€ test_tensor.cpp
â”‚   â”œâ”€â”€ test_operations.cpp
â”‚   â”œâ”€â”€ test_activations.cpp
â”‚   â””â”€â”€ test_python.py
â”œâ”€â”€ benchmarks/                 # Performance tests
â”‚   â”œâ”€â”€ tensor_benchmarks.cpp
â”‚   â””â”€â”€ operations_benchmarks.cpp
â”œâ”€â”€ examples/                   # Usage examples
â”‚   â”œâ”€â”€ basic_tensor_ops.py
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â””â”€â”€ linear_regression.py
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ api/                    # API documentation
â”‚   â”œâ”€â”€ tutorials/              # Tutorial notebooks
â”‚   â””â”€â”€ internals/              # Internal implementation docs
â”œâ”€â”€ scripts/                    # Build and utility scripts
â”‚   â”œâ”€â”€ build.sh
â”‚   â”œâ”€â”€ test.sh
â”‚   â””â”€â”€ benchmark.sh
â”œâ”€â”€ CMakeLists.txt              # Main CMake configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- C++17 or later
- CMake 3.15+
- Python 3.8+
- BLAS library (OpenBLAS, Intel MKL, or ATLAS)
- pybind11 (for Python bindings)

### Building from Source

```bash
# Clone the repository
git clone https://github.com/yourusername/tensorcore.git
cd tensorcore

# Create build directory
mkdir build && cd build

# Configure with CMake
cmake .. -DCMAKE_BUILD_TYPE=Release

# Build the library
make -j$(nproc)

# Run tests
make test

# Install Python bindings
cd ../python
pip install -e .
```

### Quick Start Example

```cpp
#include "tensorcore/tensor.hpp"
#include "tensorcore/layers.hpp"
#include "tensorcore/optimizers.hpp"
#include "tensorcore/autograd.hpp"
#include "tensorcore/simd_utils.hpp"

using namespace tensorcore;

int main() {
    // Create tensors with automatic differentiation
    auto x = variable(Tensor({1, 2}, {1.0, 2.0}), true);
    auto y = variable(Tensor({1, 2}, {3.0, 4.0}), true);
    
    // Perform operations with gradient tracking (SIMD-optimized)
    auto z = global_graph.add(x, y);
    auto result = global_graph.multiply(z, z);
    
    // Compute gradients
    global_graph.backward(result);
    
    // Create a neural network with Conv2D
    auto conv1 = std::make_shared<Conv2D>(1, 32, 3, 1, 1, true, "relu");
    auto pool1 = std::make_shared<MaxPool2D>(2, 2, 0);
    auto dense1 = std::make_shared<Dense>(32 * 13 * 13, 128, true, "relu");
    auto dense2 = std::make_shared<Dense>(128, 10, true, "softmax");
    
    Sequential network({conv1, pool1, dense1, dense2});
    
    // Forward pass with SIMD optimizations
    Tensor input({1, 1, 28, 28}); // MNIST-like image
    Tensor output = network.forward(input);
    
    // Training with optimizer
    Adam optimizer(0.001);
    optimizer.add_parameters(network.get_parameters());
    
    // Training loop
    for (int epoch = 0; epoch < 100; ++epoch) {
        Tensor pred = network.forward(input);
        // Compute loss and gradients...
        network.backward(grad_output);
        optimizer.step();
        network.zero_grad();
    }
    
    return 0;
}
```

## ğŸš€ Core Features Implemented

### **Automatic Differentiation System**
- Complete computational graph with forward/backward passes
- Support for complex mathematical expressions
- Efficient gradient computation using chain rule
- Memory-efficient graph construction and cleanup

### **Neural Network Components**
- **Dense Layer**: Fully connected layer with Xavier initialization
- **Conv2D Layer**: 2D convolution with forward/backward passes
- **MaxPool2D Layer**: Maximum pooling for downsampling
- **AvgPool2D Layer**: Average pooling for downsampling
- **Activation Functions**: ReLU, Sigmoid, Tanh with proper gradients
- **Sequential Container**: Easy multi-layer network construction
- **Gradient Flow**: End-to-end gradient computation through layers

### **Performance Optimizations**
- **SIMD Vectorization**: AVX2/AVX/SSE instructions for 4x-8x speedup
- **Memory Pool**: Efficient allocation/deallocation for large tensors
- **CPU Feature Detection**: Automatic detection of available SIMD instructions
- **Optimized Operations**: All tensor operations use vectorized instructions

### **Optimization Algorithms**
- **SGD**: Stochastic Gradient Descent with momentum
- **Adam**: Adaptive learning rate optimization
- **AdamW**: Adam with decoupled weight decay
- **RMSprop**: Root Mean Square propagation
- **Adagrad**: Adaptive gradient algorithm
- **Adadelta**: Adagrad with moving average
- **Adamax**: Adam with infinity norm
- **Nadam**: Nesterov-accelerated Adam

### **Testing & Validation**
- Comprehensive test suite with 100% core functionality coverage
- End-to-end neural network training verification
- Gradient computation accuracy validation
- Performance benchmarking framework
- SIMD optimization testing

## ğŸ§® Mathematical Foundations

This library implements the core mathematical concepts behind machine learning:

### Linear Algebra
- Matrix operations (multiplication, addition, transposition)
- Vector operations (dot product, cross product)
- Eigenvalue decomposition
- Singular Value Decomposition (SVD)

### Calculus
- Automatic differentiation
- Gradient computation
- Chain rule implementation
- Backpropagation algorithms

### Statistics
- Probability distributions
- Statistical moments
- Sampling methods
- Hypothesis testing utilities

## ğŸ”§ Development

### Running Tests

```bash
# Build and run core functionality tests
mkdir build && cd build
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/tensor.cpp -o tensor.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/operations.cpp -o operations.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/autograd.cpp -o autograd.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/layers.cpp -o layers.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/optimizers.cpp -o optimizers.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/activations.cpp -o activations.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/simd_utils.cpp -o simd_utils.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../src/memory_pool.cpp -o memory_pool.o
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../test_core_functionality.cpp -o test_core.o
g++ -std=c++17 -mavx2 -mfma -o test_core test_core.o tensor.o operations.o autograd.o layers.o optimizers.o activations.o simd_utils.o memory_pool.o
./test_core

# Run SIMD performance tests
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../test_simd_performance.cpp -o test_simd.o
g++ -std=c++17 -mavx2 -mfma -o test_simd test_simd.o tensor.o operations.o autograd.o layers.o optimizers.o activations.o simd_utils.o memory_pool.o
./test_simd

# Run Conv2D tests
g++ -std=c++17 -mavx2 -mfma -I ../include -c ../test_conv2d.cpp -o test_conv2d.o
g++ -std=c++17 -mavx2 -mfma -o test_conv2d test_conv2d.o tensor.o operations.o autograd.o layers.o optimizers.o activations.o simd_utils.o memory_pool.o
./test_conv2d

# Expected output:
# Testing TensorCore Core Functionality
# =====================================
# Testing basic autograd...
# âœ“ Basic autograd test passed
# Testing Dense layer...
# âœ“ Dense layer test passed
# Testing SGD optimizer...
# âœ“ Optimizer test passed
# Testing simple neural network...
# âœ“ Simple neural network test passed
# ğŸ‰ All core functionality tests passed!
```

### Benchmarking

```bash
# Run SIMD performance benchmarks
./test_simd

# Run Conv2D performance tests
./test_conv2d

# Run core functionality tests
./test_core

# Compare with NumPy (when Python bindings are ready)
python benchmarks/compare_with_numpy.py
```

### Code Style

- Follow Google C++ Style Guide
- Use clang-format for code formatting
- Write comprehensive unit tests
- Document all public APIs

## ğŸ“š Learning Resources

### **ğŸ“ Educational Guides**
- **[Educational Concepts & Theory](README_EDUCATIONAL.md)** - Deep dive into ML/DL concepts and why these libraries exist
- **[Development Roadmap](README_ROADMAP.md)** - Complete roadmap of pending features for a production-ready library
- **[Getting Started Tutorial](docs/tutorials/getting_started.md)** - Step-by-step introduction to TensorCore

### **ğŸ“– Technical Documentation**
- [Linear Algebra for Machine Learning](docs/tutorials/linear_algebra.md)
- [Understanding Automatic Differentiation](docs/tutorials/autograd.md)
- [SIMD Optimizations Explained](docs/internals/simd_optimizations.md)
- [Memory Management in C++](docs/internals/memory_management.md)

### **ğŸ”¬ API Documentation**
- [Tensor Creation Functions](docs/api/tensor_creation.md)
- [Mathematical Operations](docs/api/mathematical_functions.md)
- [Activation Functions](docs/api/activation_functions.md)
- [Loss Functions](docs/api/loss_functions.md)
- [Complete API Reference](docs/api/README.md)

## ğŸ¤ Contributing

This is an educational project! Contributions are welcome, especially:

- Implementation of new mathematical operations
- Performance optimizations
- Educational examples and tutorials
- Documentation improvements
- Bug fixes and code quality improvements

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by NumPy, PyTorch, and TensorFlow
- Built with pybind11 for Python integration
- Uses OpenBLAS for linear algebra operations
- Educational resources from various ML courses and textbooks

## ğŸ“ Educational Value

By building this library, you'll learn:

1. **Core Mathematics**: How linear algebra and calculus power ML algorithms
2. **Memory Management**: Efficient data structures and memory allocation
3. **Performance Optimization**: SIMD, vectorization, and parallel computing
4. **API Design**: How to create intuitive and efficient interfaces
5. **Python Integration**: Bridging high-level Python with low-level C++
6. **Testing and Benchmarking**: Ensuring correctness and performance

---

**Happy Learning! ğŸš€**

*This library is designed to be your gateway into understanding the beautiful mathematics and engineering behind modern machine learning frameworks.*